qqnorm(resi_simple)
qqline(resi_simple)
plot(reg_simple) # Normal QQ, Residuals vs. Leverage, Scale Location
plot(density(simple_linear_residuals), main = "Residuals for the Simple Linear Regression Model")
simple_linear <- lm(Fertility ~ Education, data = swiss)
simple_linear_residuals <- resid(simple_linear)
plot(density(simple_linear_residuals), main = "Residuals for the Simple Linear Regression Model")
plot(density(resi_simple), main = "Residuals for the Simple Linear Regression Model")
library(lmtest)
library(lmtest)
bptest(reg_simple)
library(lmtest)
bptest(simple_linear)
Education2 = Education^2
Education3 = Education^3
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
print(reg_simple2)
print(reg_simple3)
summary(reg_simple2)
summary(reg_simple3)
simple_quadratic <- lm(Fertility ~ Education  + Education2, data = swiss)
simple_cubic <- lm(Fertility ~ Education + Education2 + Education3, data = swiss)
summary(simple_quadratic)
summary(simple_cubic)
fit_matrix <- matrix(NA, nrow = 3, ncol = 2)
colnames(fit_matrix) <- c("R sq.", "Adj. R sq.")
rownames(fit_matrix) <- c("Linear", "Quadratic", "Cubic")
fit_matrix[1,1] <- summary(reg_simple)$r.squared
fit_matrix[1,2] <- summary(reg_simple)$adj.r.squared
fit_matrix[2,1] <- summary(reg_simple2)$r.squared
fit_matrix[2,2] <- summary(reg_simple2)$adj.r.squared
fit_matrix[3,1] <- summary(reg_simple3)$r.squared
fit_matrix[3,2] <- summary(reg_simple3)$adj.r.squared
fit_matrix
plot(Education, reg_simple2$fitted);
lines(Education, reg_simple$fitted, col = "red") # if they are similar, the model should be linear, if the deviation is small the linear model is not wrong
plot(Education, reg_simple3$fitted.values, col = "green", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red")
plot(Education, reg_simple2$fitted);
lines(Education, reg_simple$fitted, col = "red") # if they are similar, the model should be linear, if the deviation is small the linear model is not wrong
plot(Education, reg_simple3$fitted.values, col = "green", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red")
bandwith <- npregbw(Fertility ~ Education)
summary(bandwith)
npreg1 <- npreg(bws = bandwidth)
bandwidth <- npregbw(Fertility ~ Education)
summary(bandwidth) # the optimal bandwidth is 6.76351
# We then estimate the function using the optimal bandwidth
npreg1 <- npreg(bws = bandwidth)
plot(bandwith, plot.errors.method = "bootstrap")
lines(Education, simple_linear$fitted.values, col = "red")
plot(bandwith, plot.errors.method = "bootstrap", main = "Non-parametric regression")
lines(Education, simple_linear$fitted.values, col = "red")
k <- 9
sigma_simple <- vector(length = k)
sigma_simple2 <- vector(length = k)
sigma_simple3 <- vector(length = k)
for(i in 1:k){
ind <- rep(0, times = 47)
a <- (i - 1) * 5 + 1
b <- i * 5
ind[a : b] = 1
Test.F = swiss$Fertility[ind == 1]
Test.E = swiss$Education[ind == 1]
Train.F = swiss$Fertility[ind == 0]
Train.E = swiss$Education[ind == 0]
# Defining the test sets
Train.E2 = Train.E^2
Train.E3 = Train.E^3
#Fitting the training data
reg_linear <- lm(Train.F ~ Train.E)
reg_quadratic <- lm(Train.F ~ Train.E + Train.E2)
reg_cubic <- lm(Train.F ~ Train.E + Train.E2 + Train.E3)
#Extract the fitted values
coef.1 <- reg_linear$coef
coef.2 <- reg_quadratic$coef
coef.3 <- reg_cubic$coef
#Calculate SSR on dataset
sigma_simple[i] <- sum((Test.F - coef.1[1] - coef.1[2] * Test.E)^2)
sigma_simple2[i] <- sum((Test.F - coef.2[1] - coef.2[2] * Test.E - coef.2[3] * (Test.E^2))^2)
sigma_simple3[i] <- sum((Test.F - coef.3[1] - coef.3[2] * Test.E - coef.3[3] * (Test.E^2) - coef.3[4] * (Test.E^3))^2)
}
# Average SSR for all three models
avg_sigma_simple <- mean(sigma_simple)
avg_sigma_simple2 <- mean(sigma_simple2)
avg_sigma_simple3 <- mean(sigma_simple3)
avg_sigma_simple
avg_sigma_simple2
avg_sigma_simple3
# Creating a matrix so store the values
cross_validation_matrix <- matrix(NA, nrow = 3, ncol = 1)
colnames(cross_validation_matrix) <- c("Average SSR")
rownames(cross_validation_matrix) <- c("Linear", "Quadratic", "Cubic")
cross_validation_matrix[1] <- avg_sigma_simple
cross_validation_matrix[2] <- avg_sigma_simple2
cross_validation_matrix[3] <- avg_sigma_simple3
cross_validation_matrix
resettest(reg_simple, power = 2:3, type = "regressor")
plot(Fertility ~ Education, data = mydata, col = "grey", pch = 20, main = "Simple Model")
fit_1 = lm(Fertility ~ Education, data = mydata)
abline(fit_1, col = "green", lwd = 3)
plot(fitted(fit_1), resid(fit_1), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residuals", main = "Simple Model: fitted vs. resiudals")
abline(h = 0, col = "darkorange", lwd = 2)
install.packages("lmtest")
library(lmtest)
bptest(fit_1) # large p value we dont reject h0 of homoscedasticity --> all good!
install.packages("lmtest")
bptest(fit_1) # large p value we dont reject h0 of homoscedasticity --> all good!
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)
shapiro.test(resid(fit_1))
par(mfrow = c(2,2))
plot(reg_simple)
model_eva <- leaps::regsubsets(Fertility ~ ., data = mydata, nbest = 2) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which)
par(mfrow = c(1, 2))
plot(model_eva) #the lower the bic the better the model best model includes agriculture, education, catholic and infant mortality
plot(model_eva, scale = "adjr2") # highest adj r2 all iv, second highest adjr2 includes all ivs except examination
### Diagnostics ###
par(mfrow = c(2,2))
plot(Reg_full)
# Plot 1: Residual plot, no pattern observed constant variance assumption holds
# Plot 2: Normal Q-Q plot, normality assumption holds
# Plot 3: some influential points
reg_full <- lm(Fertility ~ Agriculture + Education + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full)
par(mfrow = c(2,2))
plot(reg_full)
resi_full = reg_full$residuals
shapiro.test(resi_full) # p-value of 0.0592 so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_full) #p-value of 0.5252
library(lmtest)
install.packages("datasets.load")
install.packages("moments")
install.packages("stargazer")
install.packages("leaps")
install.packages("np")
library(datasets.load)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lattice)
library(plotly)
library(knitr)
library(MASS)
library(faraway)
library(moments)
library(stargazer)
library(leaps)
library(glmnet)
library(np)
library(lmtest)
bptest(reg_simple) #p-value of 0.5252
bptest(reg_full) #p-value of 0.5252
library(mctest)
install.packages("mctest")
library(mctest)
omcdiag(X, Fertility)
View(mydata)
X = c(Agriculture, Examination, Education, Catholic, Infant.Mortality)
omcdiag(X, Fertility)
X <- c(Agriculture, Examination, Education, Catholic, Infant.Mortality)
Agriculture
help(swiss)
View(swiss)
mydata = swiss
#### Re-naming the relevant variables for easier access ####
attach(mydata)
###################################################################################
############################ Descriptive Stats ####################################
###################################################################################
#### Getting a general overview of the data ####
summary(mydata) # Comment: Catholic: Median and Mean are completely different, also high sd (41)
stargazer(mydata, type = "html", nobs = FALSE, style = "aer", iqr = FALSE , title = "Table 1 - Swiss Fertility Summary Statistics", digits = 2, out = "Summary Statistics")
#### Drawing a boxplot for first inspection ####
boxplot(mydata, ylab = "Occurrence", main = "Boxplot of the Swiss Fertility data set") # Comment: We have to watch out with "Catholic" as it is more or less a "binary" variable
#### Empirical Distribution Function Plots ####
plot.ecdf(Fertility, xlab = "Fertility", main = "Empirical Distribution Function Fertility")
plot.ecdf(Agriculture, xlab = "Agriculture", main ="Empirical Distribution Function Agriculture")
plot.ecdf(Examination, xlab = "Examination", main = "Empirical Distribution Function Examination")
plot.ecdf(Catholic, xlab = "Catholic", main = "Empirical Distribution Function Catholic") # Comment: looks almost binary
plot.ecdf(Infant.Mortality, xlab = "Infant.Mortality", main = "Empirical Distribution Function Infant Mortality")
#### Histogram of the distribution of fertility and education across the whole dataset ####
hist(Fertility, main = "Fertility", xlab = "Fertility") # Fertility rates are mostly between 60 and 90%
hist(Education, main = "Education", xlab = "Education") # Mostly lower levels of education in the dataset
#### Density plot of Fertility as dependent variable ####
plot(density(Fertility), main = "Fertility")
abline(v=c(mean(Fertility), median(Fertility)), col="gray94") # Median and Mean close, almost normally distributed with some tail
#### Creating a covariance and correlation matrix to observe potential dependencies ####
cov(mydata)
cor_matrix = as.matrix(cor(mydata)) # correlations with response variable lower than .8.
cor_matrix[upper.tri(cor_matrix)] <- NA
print(cor_matrix, na.print = "")
## Plotting correlation matrix for improved visual inspection ##
require(lattice)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
#### General plot of all variables for preliminary assumptions regarding model fit ####
pairs(mydata, upper.panel = NULL, pch = 20, cex = 1.25) # assumption: linear relationship between Education and Examination/Examination and Agriculture
############################ Exploring the Data ####################################
#### Ranking of provinces with regards to Fertility rates ####
## Top 10 provinces with highest fertility rates ##
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
head(10)
## Top 10 provinces with lowest fertility rates ##
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
tail(10) # Cities: Geneve, Lausanne, Nyone
#### Ranking of provinces with regards to Education ####
## Top 10 provinces with high education percentage ##
mydata %>%
dplyr::select(Education) %>%
arrange(desc(Education)) %>%
head(10) # Geneve 53 (outlier)
## Top 10 provinces with lowest education rates ##
mydata %>%
dplyr::select(Education) %>%
arrange(desc(Education)) %>%
tail(10)
###################################################################################
############################## Model Analysis #####################################
###################################################################################
######################## Simple Linear Regression #################################
##### First, we run a simple linear model with the two variables of interest, Fertility and Education, with Education being the regressor ####
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
## For a first inspection, we visualize the simple regression results ##
fit = fitted(reg_simple)
plot(Education, Fertility, main = "Fertility and Education Regression", xlab = "Education", ylab = "Fertility")
lines(Education, fit, col = 2)
# Additionally, we visualize the same data with a more complex model #
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
# And with another chart, with the size of the dots equaling the underlying values #
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
xlab("Education") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Education,
y = Fertility,
color = Education,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Education,
y = Fertility),
method = "lm") +
ylim(0, 100)
## We then check the residuals to observe whether there is any pattern (homo- vs. heteroskedasticity) ##
plot(reg_simple)
# We plot the residuals for the simple model (check if X and U are not related) #
resi_simple = reg_simple$residuals
plot(mydata$Education, resi_simple, main = "Residuals from the linear regression (Fertility ~ Education)", xlab = "Education", ylab = "Residuals")
lines(mydata$Education, rep(0, times = length(mydata$Education)), col = 2) # no clear pattern can be observed in the residuals (can only be said for lower levels of Education given amount of datapoints)
# Additionally, we run a Q-Q plot to determine whether the residuals follow a normal distribution #
qqnorm(resi_simple)
qqline(resi_simple)
# Furthermore, a density plot of the residuals is inspected #
plot(density(resi_simple), main = "Residuals for the Simple Linear Regression Model")
# We check the normal distribution of the residuals with a Shapiro-Wilk test #
shapiro.test(resi_simple) # p-value of 0.0592 so we cannot reject the 0 hypothesis of normal distribution
# Lastly, a Breusch-Pagan test is applied to check whether there is heteroskedasticity in the data
bptest(reg_simple) #p-value of 0.5252
# Based on the large p-value from the Breusch-Pagan test and the visual inspection, we cannot reject the 0 hypothesis of homoskedasticity
######################### Polynomial Regression ##################################
#### To check whether a polynomial regression can increase explanatory power, we will run a quadratic and cubic regression ####
## We therefore first define the variables of interest ##
Education2 = Education^2
Education3 = Education^3
# And check the variables for multicollinearity #
cor(Education, Education2) # highly correlated 0.9361279
cor(Education, Education3) # rel. highly correlated 0.8389176
cor(Education2, Education3) # highly correlated 0.9734444
## We then run the polynomial models, once as quadratic and once as cubic ##
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
## We want to extract the R squared and adj. R squared from all the models for better comparison ##
fit_matrix <- matrix(NA, nrow = 3, ncol = 2)
colnames(fit_matrix) <- c("R sq.", "Adj. R sq.")
rownames(fit_matrix) <- c("Linear", "Quadratic", "Cubic")
fit_matrix[1,1] <- summary(reg_simple)$r.squared
fit_matrix[1,2] <- summary(reg_simple)$adj.r.squared
fit_matrix[2,1] <- summary(reg_simple2)$r.squared
fit_matrix[2,2] <- summary(reg_simple2)$adj.r.squared
fit_matrix[3,1] <- summary(reg_simple3)$r.squared
fit_matrix[3,2] <- summary(reg_simple3)$adj.r.squared
fit_matrix # we can see that the linear model offers the highest R squared
## Additionally, we want to inspect the different fits visually ##
plot(Education, reg_simple3$fitted.values, col = "blue", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red") # the small deviations of fit indicates that the linear model is sufficient to use
## Next, we run a non-parametric regression to support the visual findings ##
# We first estimate the optimal bandwidth given our dataset #
bandwidth <- npregbw(Fertility ~ Education)
summary(bandwidth) # the optimal bandwidth is 6.76351
# We then estimate the function using the optimal bandwidth #
npreg1 <- npreg(bws = bandwidth)
# We plot the results and add the linear regression to it #
plot(bandwith, plot.errors.method = "bootstrap", main = "Non-parametric regression")
lines(Education, simple_linear$fitted.values, col = "red")
## Additionally, we perform a cross validation to further support the findings that a linear regression model offers the best results ##
k <- 9
sigma_simple <- vector(length = k)
sigma_simple2 <- vector(length = k)
sigma_simple3 <- vector(length = k)
for(i in 1:k){
ind <- rep(0, times = 47)
a <- (i - 1) * 5 + 1
b <- i * 5
ind[a : b] = 1
Test.F = swiss$Fertility[ind == 1]
Test.E = swiss$Education[ind == 1]
Train.F = swiss$Fertility[ind == 0]
Train.E = swiss$Education[ind == 0]
# Defining the test sets
Train.E2 = Train.E^2
Train.E3 = Train.E^3
#Fitting the training data
reg_linear <- lm(Train.F ~ Train.E)
reg_quadratic <- lm(Train.F ~ Train.E + Train.E2)
reg_cubic <- lm(Train.F ~ Train.E + Train.E2 + Train.E3)
#Extract the fitted values
coef.1 <- reg_linear$coef
coef.2 <- reg_quadratic$coef
coef.3 <- reg_cubic$coef
#Calculate SSR on dataset
sigma_simple[i] <- sum((Test.F - coef.1[1] - coef.1[2] * Test.E)^2)
sigma_simple2[i] <- sum((Test.F - coef.2[1] - coef.2[2] * Test.E - coef.2[3] * (Test.E^2))^2)
sigma_simple3[i] <- sum((Test.F - coef.3[1] - coef.3[2] * Test.E - coef.3[3] * (Test.E^2) - coef.3[4] * (Test.E^3))^2)
}
# We then calculate the average SSR for all three models #
avg_sigma_simple <- mean(sigma_simple)
avg_sigma_simple2 <- mean(sigma_simple2)
avg_sigma_simple3 <- mean(sigma_simple3)
avg_sigma_simple
avg_sigma_simple2
avg_sigma_simple3
# Finally, we create  a matrix to store the values #
cross_validation_matrix <- matrix(NA, nrow = 3, ncol = 1)
colnames(cross_validation_matrix) <- c("Average SSR")
rownames(cross_validation_matrix) <- c("Linear", "Quadratic", "Cubic")
cross_validation_matrix[1] <- avg_sigma_simple
cross_validation_matrix[2] <- avg_sigma_simple2
cross_validation_matrix[3] <- avg_sigma_simple3
cross_validation_matrix
## Lastly, we run a Ramsey RESET test for functional form ##
resettest(reg_simple, power = 2:3, type = "regressor") # p-value of 61.2% suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
## Based on the previous findings, it can be concluded that the linear expression of the regressor seems reasonable
reg_full = lm(Fertility ~ Agriculture + Education + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
par(mfrow = c(2,2))
plot(reg_full) # There does not seem to be any heteroskedasticity in the model given the visual inspection of the residuals
resi_full = reg_full$residuals
shapiro.test(resi_full) # p-value of 0.9318, so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_full) #p-value of 0.321 lends support for homoskedasticity in the model
X <- c(Agriculture, Examination, Education, Catholic, Infant.Mortality)
omcdiag(X, Fertility)
X <- mydata[, 3:7]
omcdiag(X, Fertility)
X <- mydata[,3:7]
X <- mydata[, 3:7]
?vif
install.packages("faraway")
detach("package:faraway", unload = TRUE)
install.packages("faraway")
library(faraway)
vif(reg_full)
require(sjmisc)
require(sjPlot)
?lattice
detach("package:lattice", unload = TRUE)
require(lattice)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
install.packages("lattice")
detach("package:lattice", unload = TRUE)
require(lattice)
install.packages("lattice")
library(lattice)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
install.packages("sjmisc")
install.packages("sjPlot")
library(sjmisc)
library(sjPlot)
sjp.lm(reg_full, type = "ma")
?sjp.lm
??sjp.lm
detach("package:sjPlot", unload = TRUE)
detach("package:sjmisc", unload = TRUE)
ols_vif_tol(reg_full)
install.packages("olsrr")
library(olsrr)
ols_vif_tol(reg_full)
ols_correlations(reg_full)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
play = lm(Fertility ~ Agriculture + Education + Examination)
summary(play)
play = lm(Fertility ~ Education + Examination + Catholic + Infant.Mortality)
summary(play)
ols_plot_obs_fit(reg_full)
ols_plot_diagnostics(reg_full)
ols_plot_comp_plus_resid(reg_full)
ols_plot_added_variable(reg_full)
install.packages("corrplot")
library(corrplot)
corrplot(cor(mydata), lower.col = "black", number.cex = 0.7)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
corrplot(cor(mydata), lower.col = "black", number.cex = 0.7)
pairs(mydata, upper.panel = NULL, pch = 20, cex = 1.25) # assumption: linear relationship between Education and Examination/Examination and Agriculture
ols_plot_obs_fit(reg_full)
ols_plot_diagnostics(reg_full)
plot(reg_simple)
ols_plot_diagnostics(reg_simple)
ols_plot_obs_fit(reg_full)
par(mfrow = c(2,2))
plot(reg_full) # There does not seem to be any heteroskedasticity in the model given the visual inspection of the residuals
ols_plot_diagnostics(reg_full)
ols_plot_obs_fit(reg_full)
ols_plot_obs_fit(reg_full)
ols_plot_obs_fit(reg_simple)
ols_plot_added_variable(reg_full)
ols_plot_comp_plus_resid(reg_full)
step1 <- lm(Fertility ~ 1, data = mydata) # using only the intercept
step2 <- lm(Fertility ~ ., data = mydata) # running the full regression
step(step1, direction = "forward", scope = list(lower = step1, upper = step2))
step(step2, direction = "backward")
model_eva <- leaps::regsubsets(Fertility ~ ., nbest = 2) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which)
par(mfrow = c(1, 2))
plot(model_eva) #the lower the bic the better the model best model includes agriculture, education, catholic and infant mortality
plot(model_eva, scale = "adjr2") # highest adj r2 all iv, second highest adjr2 includes all ivs except examination
plot(model_eva, main ="Comparison of goodness of different models") #the lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
plot(model_eva, ylab = "BIC", main ="Comparison of goodness of different models") #the lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
plot(model_eva, scale = "AdjR2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
plot(model_eva, main ="Comparison of goodness of different models") #the lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
ols_plot_comp_plus_resid(reg_full) #Given the current dataset, there seems to be no need for any possible transformations
ols_vif_tol(reg_full) # Based on the given values which are all below threshold of 4 / 5 and especially 10, there does not seem to be a problem with multicollinearity in the model
ols_plot_diagnostics(reg_full) # The residuals seem to be normally distributed and no heteroskedasticity present
install.packages("ggfortify")
library(ggfortify)
autoplot(reg_full)
par(mfrow = c(2,2))
plot(reg_full) # There does not seem to be any heteroskedasticity in the model given the visual inspection of the residuals
autoplot(reg_full)
install.packages("GGally")
library(GGally)
ggpairs(mydata)
pairs(mydata, upper.panel = NULL, pch = 20, cex = 1.25) # assumption: linear relationship between Education and Examination/Examination and Agriculture
ggpairs(mydata)
corrplot(cor(mydata), lower.col = "black", number.cex = 0.7)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
corrplot(cor(mydata), lower.col = "black", number.cex = 0.7)
mydata[cooks.distance(Reg_full) > 0.1,] # Sierre, Rive Gauche, Porrentruy, Neuchatel, Rive Droite, Rive Gauche
mydata[cooks.distance(reg_full) > 0.1,] # Sierre, Rive Gauche, Porrentruy, Neuchatel, Rive Droite, Rive Gauche
ols_plot_diagnostics(reg_full) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_full) > 0.1,] # Sierre, Rive Gauche, Porrentruy, Neuchatel, Rive Droite, Rive Gauche are influential points according to the Cook's Distance parameter
corrplot(cor(mydata), method = "color")
corrplot(cor(mydata), method = "color")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Agriculture, y = Fertility)) +
geom_smooth(mapping = aes(x = Agriculture, y = Fertility),
method = "lm")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mydata), method = "color", col = col(200),
type = "upper", order = "hclust",
addCoef.col = "black",
tl.col = "black", tl.srt = 45,
p.mat = p.mat, sig.level = 0.01, insig = "blank",
diag = FALSE)
corrplot(cor(mydata), method = "color")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Agriculture, y = Fertility)) +
geom_smooth(mapping = aes(x = Agriculture, y = Fertility),
method = "lm")
ggpairs(mydata)
ols_plot_added_variable(reg_full)
ols_plot_added_variable(reg_full)
reg_woEx = lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
ols_plot_obs_fit(reg_woEx)
ols_plot_obs_fit(reg_full) # The model seems to fit pretty well based on this visual inspection
ols_plot_diagnostics(reg_woEx) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_woEx) > 0.1,] # Porrentruy, Sierre, Neuchatel, Rive Droite and Rive Gauche are influential points according to the Cook's Distance parameter
bptest(reg_woEx)
resi_woEx = reg_woEx$residuals
shapiro.test(resi_woEx)
ols_vif_tol(reg_woEx)
mydata2 = cbind(mydata, mydata$ReligionDummy = ifelse(mydata$Catholic >= 50, 1, 0))
