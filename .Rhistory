notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Examination", "Dummy", "Dummy Full"),
c("Additional Variables?", "N", "N", "N", "Y", "Y", "Y", "Y"),
c("Catholic Dummy?", "N", "N", "N", "N", "N", "Y", "Y")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examiniation", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Examination", "Dummy", "Dummy Full"),
c("Additional Variables?", "N", "N", "N", "Y", "Y", "Y", "Y"),
c("Catholic Dummy?", "N", "N", "N", "N", "N", "Y", "Y")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Examination", "Dummy", "Dummy Full"),
c("Additional Variables?", "N", "N", "N", "Y", "Y", "Y", "Y"),
c("Catholic Dummy?", "N", "N", "N", "N", "N", "Y", "Y")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Examination", "Dummy", "Dummy Full"),
c("Additional Variables?", "No", "No", "No", "Yes", "Yes", "Yes", "Yes"),
c("Catholic Dummy?", "No", "No", "No", "No", "No", "Yes", "Yes")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTable.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Exam.", "Dummy", "Dummy Full")),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Exam.", "Dummy", "Dummy Full"),
c("Additional Variables?", "No", "No", "No", "Yes", "Yes", "Yes", "Yes"),
c("Catholic Dummy?", "No", "No", "No", "No", "No", "Yes", "Yes")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Exam.", "Dummy", "Dummy Full")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort2.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
omit = c("Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Exam.", "Dummy", "Dummy Full"),
c("Additional Variables?", "No", "No", "No", "Yes", "Yes", "Yes", "Yes"),
c("Catholic Dummy?", "No", "No", "No", "No", "No", "Yes", "Yes")),
omit.stat = c("rsq", "f", "ser"),
notes = "SE provided in parantheses",
results = "asis")
ggplot(mydata, aes(x = Education, y = Fertility)) +
geom_bin2d() +
theme_bw()
## What does this do? Was in chapter "Linear Regression model" with the residuals check
# Plot
plot(reg_simple) # Normal QQ, Residuals vs. Leverage, Scale Location
geom_bin2d() +
theme_bw()
ggplot(mydata, aes(x = Education, y = Fertility)) +
geom_bin2d() +
theme_bw()
set.seed (20210508)
lasso_x <- as.matrix(mydata[ ,2:6])
lasso_y <- swiss$Fertility
size <- floor(0.75 * nrow(mydata)) # Generate variable with the rows in training data
training_set <- sample(seq_len(nrow(mydata)), size = size)
lasso.cv <- cv.glmnet(
lasso_x[training_set,],
lasso_y[training_set],
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso <- coef(lasso.cv, s = "lambda.min") # save for later comparison
print(coef_lasso) # It seems like none have to be dropped to be compared to the OLS
plot(lasso.cv) # Grey bars are standard deviations along the Lambda sequence
best_lambda <- lasso.cv$lambda.min
print(best_lambda)
predlasso1 <- predict(lasso.cv, newx = lasso_x[-training_set,], s = lasso.cv$lambda.min)
predMSElasso <- mean((lasso_y[-training_set] - predlasso1)^2)
print(predMSElasso)
rss <- sum((predlasso1 - lasso_y[-training_set])^2) #residual sum of squares
tss <- sum((lasso_y[-training_set] - mean(lasso_y[-training_set])) ^ 2)
rsq <- 1 - rss/tss
print(rsq)
lasso2.cv <- cv.glmnet(
lasso_x,
lasso_y,
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso2 <- coef(lasso2.cv, s = "lambda.min") # save for later comparison
print(coef_lasso2) #From a visual inspection, this does not seem to add any additional explanatory power
install.packages("datasets.load")
install.packages("moments")
install.packages("stargazer")
install.packages("leaps")
install.packages("np")
install.packages("mctest")
install.packages("faraway")
install.packages("lattice")
install.packages("olsrr")
install.packages("corrplot")
install.packages("GGally")
#### Installing the necessary libraries ####
library(datasets.load)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lattice)
library(plotly)
library(knitr)
library(MASS)
library(faraway)
library(moments)
library(stargazer)
library(leaps)
library(glmnet)
library(np)
library(lmtest)
library(mctest)
library(faraway)
library(lattice)
library(olsrr)
library(corrplot)
library(GGally)
help(swiss)
View(swiss)
mydata = swiss[, 1:6]
View(mydata)
attach(mydata)
summary(mydata) # Comment: Catholic: Median and Mean are completely different, also high sd (41)
stargazer(mydata, type = "html", nobs = FALSE, style = "aer", iqr = FALSE , title = "Table 1 - Swiss Fertility Summary Statistics", digits = 2, out = "Summary Statistics")
boxplot(mydata, ylab = "Occurrence", main = "Boxplot of the Swiss Fertility data set") # Comment: We have to watch out with "Catholic" as it is more or less a "binary" variable
?swiss
plot.ecdf(Fertility, xlab = "Fertility", main = "Empirical Distribution Function Fertility")
plot.ecdf(Agriculture, xlab = "Agriculture", main ="Empirical Distribution Function Agriculture")
plot.ecdf(Examination, xlab = "Examination", main = "Empirical Distribution Function Examination")
plot.ecdf(Catholic, xlab = "Catholic", main = "Empirical Distribution Function Catholic") # Comment: looks almost binary
plot.ecdf(Infant.Mortality, xlab = "Infant.Mortality", main = "Empirical Distribution Function Infant Mortality")
hist(Fertility, main = "Fertility", xlab = "Fertility") # Fertility rates are mostly between 60 and 90%
hist(Education, main = "Education", xlab = "Education") # Mostly lower levels of education in the dataset
plot(density(Fertility), main = "Fertility")
abline(v=c(mean(Fertility), median(Fertility)), col="gray94") # Median and Mean close, almost normally distributed with some tail
cov(mydata)
cor_matrix = as.matrix(cor(mydata)) # correlations with response variable lower than .8.
cor_matrix[upper.tri(cor_matrix)] <- NA
print(cor_matrix, na.print = "")
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
ggpairs(mydata)
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
head(10)
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
tail(10) # Cities: Geneve, Lausanne, Nyone
mydata %>%
dplyr::select(Education) %>%
arrange(desc(Education)) %>%
head(10) # Geneve 53 (outlier)
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
fit = fitted(reg_simple)
plot(Education, Fertility, main = "Fertility and Education Regression", xlab = "Education", ylab = "Fertility")
lines(Education, fit, col = 2)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
y2 = log(Fertility)
print(y2)
reg_play <- lm(y2 ~ Education, data = mydata)
summary(reg_play)
plot(reg_play)
plot(Education, y2, main = "Fertility and Education Regression", xlab = "Education", ylab = "Fertility")
lines(Education, fit, col = 2)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = y2)) +
geom_smooth(mapping = aes(x = Education, y = y2),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = y2)) +
geom_smooth(mapping = aes(x = Education, y = y2),
method = "lm") +
ylim(0, 5)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = y2)) +
geom_smooth(mapping = aes(x = Education, y = y2),
method = "lm") +
ylim(3, 5)
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
fit = fitted(reg_simple)
plot(Education, Fertility, main = "Fertility and Education Regression", xlab = "Education", ylab = "Fertility")
lines(Education, fit, col = 2)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
X <- log(Education)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = X, y = Fertility)) +
geom_smooth(mapping = aes(x = X, y = Fertility),
method = "lm") +
ylim(0, 5)
(Education)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = X, y = Fertility)) +
geom_smooth(mapping = aes(x = X, y = Fertility),
method = "lm") +
ylim(0, 100)
reg_play = lm(Fertility ~ X, data=mydata)
tility ~ X, data=mydata)
summary(reg_play)
X <- -log(Education)
reg_play = lm(Fertility ~ X, data=mydata)
summary(reg_play)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = X, y = Fertility)) +
geom_smooth(mapping = aes(x = X, y = Fertility),
method = "lm") +
ylim(0, 100)
ols_plot_obs_fit(reg_simple) # The fit does not seem completely off, but could be further improved
plot(reg_simple)
ols_plot_obs_fit(reg_simple) # The fit does not seem completely off, but could be further improved
ols_plot_diagnostics(reg_simple)
resi_simple = reg_simple$residuals
plot(mydata$Education, resi_simple, main = "Residuals from the linear regression (Fertility ~ Education)", xlab = "Education", ylab = "Residuals")
lines(mydata$Education, rep(0, times = length(mydata$Education)), col = 2) # no clear pattern can be observed in the residuals (can only be said for lower levels of Education given amount of datapoints)
qqnorm(resi_simple)
qqline(resi_simple)
shapiro.test(resi_simple) # p-value of 0.0592 so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_simple) #p-value of 0.5252
Education2 = Education^2
Education3 = Education^3
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
reg_woEx = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
swiss$CatholicDummy = ifelse(swiss$Catholic >= 50, 1, 0)
View(swiss)
reg_simple_Dummy = lm(Fertility ~ Education + CatholicDummy, data = swiss)
summary(reg_simple_Dummy)
reg_woEx_Dummy = lm(Fertility ~ Agriculture + Education + Infant.Mortality + CatholicDummy, data = swiss)
summary(reg_woEx_Dummy)
stargazer(reg_simple, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTable.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis of Education on Fertility",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility (in %)",
dep.var.labels.include = FALSE,
covariate.labels = c("Constant", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "CatholicDummy"),
column.labels = c("Simple", "Multivariate"),
column.separate = c(3, 4),
add.lines = list(c("Model", "Linear", "Quadratic", "Cubic", "Full", "w/o Exam.", "Dummy", "Dummy Full")),
notes = "SE provided in parantheses",
results = "asis")
library(datasets.load)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lattice)
library(plotly)
library(knitr)
library(MASS)
library(faraway)
library(moments)
library(stargazer)
library(leaps)
library(glmnet)
library(np)
library(lmtest)
library(mctest)
library(faraway)
library(lattice)
library(olsrr)
library(corrplot)
library(GGally)
help(swiss)
View(swiss)
mydata = swiss[, 1:6]
View(mydata)
attach(mydata)
summary(mydata) # Comment: Catholic: Median and Mean are completely different, also high sd (41)
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture (Instrumental Variable?)
ggpairs(mydata)
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
ols_plot_diagnostics(reg_simple)
ols_plot_diagnostics(reg_simple)
ols_plot_diagnostics(reg_simple)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
Education2 = Education^2
Education3 = Education^3
# And check the variables for multicollinearity #
cor(Education, Education2) # highly correlated 0.9361279
cor(Education, Education3) # rel. highly correlated 0.8389176
cor(Education2, Education3) # highly correlated 0.9734444
## We then run the polynomial models, once as quadratic and once as cubic ##
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
resettest(reg_simple, power = 2:3, type = "regressor") # p-value of 61.2% suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
fit_matrix <- matrix(NA, nrow = 3, ncol = 2)
colnames(fit_matrix) <- c("R2", "AdjR2")
rownames(fit_matrix) <- c("Linear", "Quadratic", "Cubic")
fit_matrix[1,1] <- summary(reg_simple)$r.squared
fit_matrix[1,2] <- summary(reg_simple)$adj.r.squared
fit_matrix[2,1] <- summary(reg_simple2)$r.squared
fit_matrix[2,2] <- summary(reg_simple2)$adj.r.squared
fit_matrix[3,1] <- summary(reg_simple3)$r.squared
fit_matrix[3,2] <- summary(reg_simple3)$adj.r.squared
fit_matrix # we can see that the linear model offers the highest R squared
plot(Education, reg_simple3$fitted.values, col = "blue", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red") # the small deviations of fit indicates that the linear model is sufficient to use
bandwidth <- npregbw(Fertility ~ Education)
summary(bandwidth) # the optimal bandwidth is 6.76351
# We then estimate the function using the optimal bandwidth #
npreg1 <- npreg(bws = bandwidth)
# We plot the results and add the linear regression to it #
plot(bandwith, plot.errors.method = "bootstrap", main = "Non-parametric regression")
lines(Education, simple_linear$fitted.values, col = "red")
## Additionally, we want to inspect the different fits visually ##
plot(Education, reg_simple3$fitted.values, col = "blue", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red") # the small deviations of fit indicates that the linear model is sufficient to use
resettest(reg_woEx, power = 2:3, type = "regressor") # p-value of 61.2% suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
ols_plot_obs_fit(reg_full) # The model seems to fit pretty well based on this visual inspection
ols_vif_tol(reg_full) # Based on the given values which are all below threshold of 4 / 5 and especially 10, there does not seem to be a problem with multicollinearity in the model
step1 <- lm(Fertility ~ 1, data = mydata) # using only the intercept
step2 <- lm(Fertility ~ ., data = mydata) # running the full regression
# Next, we run the forward and backward regression #
step(step1, direction = "forward", scope = list(lower = step1, upper = step2))
step(step2, direction = "backward") # Both methods yield the same result: the best model does not include the variable "Examination"
# We then plot the results for better visual inspection #
model_eva <- leaps::regsubsets(Fertility ~ ., nbest = 2) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which) # it seems to become evident that including both variables Education and Examination decreases the goodness of the model
# We can further visualize the BIC factor and AdjR2
par(mfrow = c(1, 2))
plot(model_eva, main ="Comparison of goodness of different models") #the lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
reg_woEx = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
ols_plot_obs_fit(reg_woEx) # again, the fit seems to be pretty well
# We run a reduced model diagnostics, not expecting any different results from before #
ols_plot_diagnostics(reg_woEx) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_woEx) > 0.1,] # Only Porrentruy, Sierre and Rive Gauche are now influential points according to the Cook's Distance parameter
# We run the Shapiro-Wilk and Breusch-Pagan test #
resi_woEx = reg_woEx$residuals
shapiro.test(resi_woEx) # large p value, therefore normal distribution of residuals
bptest(reg_woEx) # high p value, therefore no heteroskedasticity
# We check once more for multicollinearity between the variables #
ols_vif_tol(reg_woEx) # Unsurprisingly, there is no multicollinearity here either
# Lastly, we run a Ramsey RESET test for functional form #
resettest(reg_woEx, power = 2:3, type = "regressor") # p-value of 0.6311 suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
