Education3 = Education^3
cor(Education, Education2) # highly correlated 0.9361279
cor(Education, Education3) # rel. highly correlated 0.8389176
cor(Education2, Education3) # highly correlated 0.9734444
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
fit_matrix <- matrix(NA, nrow = 3, ncol = 2)
colnames(fit_matrix) <- c("R2", "AdjR2")
rownames(fit_matrix) <- c("Linear", "Quadratic", "Cubic")
fit_matrix[1,1] <- summary(reg_simple)$r.squared
fit_matrix[1,2] <- summary(reg_simple)$adj.r.squared
fit_matrix[2,1] <- summary(reg_simple2)$r.squared
fit_matrix[2,2] <- summary(reg_simple2)$adj.r.squared
fit_matrix[3,1] <- summary(reg_simple3)$r.squared
fit_matrix[3,2] <- summary(reg_simple3)$adj.r.squared
fit_matrix # we can see that the linear model offers the highest Adj. R squared
plot(Education, reg_simple3$fitted.values, col = "blue", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red") # the small deviations of fit indicates that the linear model is sufficient to use
bandwidth <- npregbw(Fertility ~ Education)
summary(bandwidth) # the optimal bandwidth is 6.76351
npreg1 <- npreg(bws = bandwidth)
plot(bandwidth, plot.errors.method = "bootstrap", main = "Non-parametric regression")
lines(Education, reg_simple$fitted.values, col = "red")
k <- 9
sigma_simple <- vector(length = k)
sigma_simple2 <- vector(length = k)
sigma_simple3 <- vector(length = k)
for(i in 1:k){
ind <- rep(0, times = 47)
a <- (i - 1) * 5 + 1
b <- i * 5
ind[a : b] = 1
Test.F = swiss$Fertility[ind == 1]
Test.E = swiss$Education[ind == 1]
Train.F = swiss$Fertility[ind == 0]
Train.E = swiss$Education[ind == 0]
# Defining the test sets
Train.E2 = Train.E^2
Train.E3 = Train.E^3
#Fitting the training data
reg_linear <- lm(Train.F ~ Train.E)
reg_quadratic <- lm(Train.F ~ Train.E + Train.E2)
reg_cubic <- lm(Train.F ~ Train.E + Train.E2 + Train.E3)
#Extract the fitted values
coef.1 <- reg_linear$coef
coef.2 <- reg_quadratic$coef
coef.3 <- reg_cubic$coef
#Calculate SSR on dataset
sigma_simple[i] <- sum((Test.F - coef.1[1] - coef.1[2] * Test.E)^2)
sigma_simple2[i] <- sum((Test.F - coef.2[1] - coef.2[2] * Test.E - coef.2[3] * (Test.E^2))^2)
sigma_simple3[i] <- sum((Test.F - coef.3[1] - coef.3[2] * Test.E - coef.3[3] * (Test.E^2) - coef.3[4] * (Test.E^3))^2)
}
# We then calculate the average SSR for all three models #
avg_sigma_simple <- mean(sigma_simple)
avg_sigma_simple2 <- mean(sigma_simple2)
avg_sigma_simple3 <- mean(sigma_simple3)
avg_sigma_simple
avg_sigma_simple2
avg_sigma_simple3
# Finally, we create  a matrix to store the values #
cross_validation_matrix <- matrix(NA, nrow = 3, ncol = 1)
colnames(cross_validation_matrix) <- c("Average SSR")
rownames(cross_validation_matrix) <- c("Linear", "Quadratic", "Cubic")
cross_validation_matrix[1] <- avg_sigma_simple
cross_validation_matrix[2] <- avg_sigma_simple2
cross_validation_matrix[3] <- avg_sigma_simple3
cross_validation_matrix
resettest(reg_simple, power = 2:3, type = "regressor") # p-value of 61.2% suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
ols_plot_obs_fit(reg_full) # The model seems to fit pretty well based on this visual inspection
ols_plot_diagnostics(reg_full) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_full) > 0.1,] # Porrentruy, Sierre, Neuchatel, Rive Droite and Rive Gauche are influential points according to the Cook's Distance parameter
resi_full = reg_full$residuals
shapiro.test(resi_full) # p-value of 0.9318, so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_full) #p-value of 0.321 lends support for homoskedasticity in the model
ols_vif_tol(reg_full) # Based on the given values which are all below threshold of 4 / 5 and especially 10, there does not seem to be a problem with multicollinearity in the model
resettest(reg_full, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
step1 <- lm(Fertility ~ 1, data = mydata) # using only the intercept
step2 <- lm(Fertility ~ ., data = mydata) # running the full regression
step(step1, direction = "forward", scope = list(lower = step1, upper = step2))
step(step2, direction = "backward") # Both methods yield the same result: the best model does not include the variable "Examination"
model_eva <- leaps::regsubsets(Fertility ~ ., nbest = 2, data = mydata) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which) # it seems to become evident that including both variables Education and Examination decreases the goodness of the model
par(mfrow = c(1, 2))
plot(model_eva, main ="Comparison of goodness") # The lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
ols_plot_added_variable(reg_full) # The variable Examination does not seem to have a large contribution to the model
reg_woEx = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
ols_plot_obs_fit(reg_woEx) # again, the fit seems to be pretty well
ols_plot_diagnostics(reg_woEx) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_woEx) > 0.1,] # Only Porrentruy, Sierre and Rive Gauche are now influential points according to the Cook's Distance parameter
resi_woEx = reg_woEx$residuals
shapiro.test(resi_woEx) # large p value, therefore normal distribution of residuals
bptest(reg_woEx) # high p value, therefore no heteroskedasticity
ols_vif_tol(reg_woEx) # Unsurprisingly, there is no multicollinearity here either
resettest(reg_woEx, power = 2:3, type = "regressor") # p-value of 0.6311 suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
boxcox(reg_woEx) # The model seems to be accurately approximated, no need for transformation of the dependent variable visible
reg_interact = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact)
ols_plot_diagnostics(reg_interact) # Residuals seem normally distributed and no signs of heteroskedasticity
resi_interact = reg_interact$residuals
shapiro.test(resi_interact) # Large p-value of 0.3102 supports normal distribution of residuals
bptest(reg_interact) # No signs of heteroskedasticity
ols_vif_tol(reg_interact) # Only interaction term that shows some weak signs of multicollinearity, therefore no cause for concern
resettest(reg_interact, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
boxcox(reg_interact) # Based on visual inspection (low Lambda value), we decide to transform the dependent variable using the log
reg_interact_transformed = lm(log(Fertility) ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact_transformed)
ols_plot_obs_fit(reg_interact_transformed) # Fit seems to be rather good for this model
ols_plot_diagnostics(reg_interact_transformed) # Residuals seem normally distributed and no signs of heteroskedasticity
shapiro.test(resi_interact_transformed) # The p-value of 0.5451 supports normal distribution of residuals
bptest(reg_interact_transformed) # No signs of heteroskedasticity
resettest(reg_interact_transformed, power = 2:3, type = "regressor") # Large p-value does also not indicate a need for polynomial transformation of any regressors
resi_interact_transformed = reg_interact_transformed$residuals
shapiro.test(resi_interact_transformed) # The p-value of 0.5451 supports normal distribution of residuals
bptest(reg_interact_transformed) # No signs of heteroskedasticity
resettest(reg_interact_transformed, power = 2:3, type = "regressor") # Large p-value does also not indicate a need for polynomial transformation of any regressors
swiss$CatholicDummy = ifelse(swiss$Catholic >= 50, 1, 0)
View(swiss)
reg_simple_Dummy = lm(Fertility ~ Education + CatholicDummy, data = swiss)
summary(reg_simple_Dummy)
reg_woEx_Dummy = lm(Fertility ~ Agriculture + Education + Infant.Mortality + CatholicDummy, data = swiss)
summary(reg_woEx_Dummy)
stargazer(reg_simple, reg_simple_transformed, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_interact, reg_interact_transformed, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTable.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(2, 6, 2),
add.lines = list(c("Model", "Linear", "Log Y", "Quadratic", "Cubic", "Full", "w/o Exam.", "Interact", "Int. / Log Y", "Dummy", "Dummy Full"),
c("AIC", round(AIC(reg_simple), 1), "", round(AIC(reg_simple2), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), "", round(AIC(reg_simple_Dummy), 1), round(AIC(reg_woEx_Dummy), 1)),
c("BIC", round(BIC(reg_simple), 1), "", round(BIC(reg_simple2), 1), round(BIC(reg_simple3), 1), round(BIC(reg_full), 1), round(BIC(reg_woEx), 1), round(BIC(reg_interact), 1), "", round(BIC(reg_simple_Dummy), 1), round(BIC(reg_woEx_Dummy), 1))),
notes = "SE provided in parentheses",
results = "asis")
stargazer(reg_simple, reg_simple3, reg_full, reg_woEx, reg_interact, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(1, 4, 1),
add.lines = list(c("AIC", round(AIC(reg_simple), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), round(AIC(reg_woEx_Dummy), 1))),
omit.stat = c("rsq", "n", "ser"),
df = F,
notes = "Based on 47 observations. SE provided in parentheses",
results = "asis")
set.seed (20210508)
lasso_x <- as.matrix(mydata[ ,2:6])
lasso_y <- swiss$Fertility
size <- floor(0.75 * nrow(mydata)) # Generate variable with the rows in training data
training_set <- sample(seq_len(nrow(mydata)), size = size)
lasso.cv <- cv.glmnet(
lasso_x[training_set,],
lasso_y[training_set],
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso <- coef(lasso.cv, s = "lambda.min") # save for later comparison
print(coef_lasso) # It seems like none have to be dropped to be compared to the OLS
plot(lasso.cv) # Grey bars are standard deviations along the Lambda sequence
best_lambda <- lasso.cv$lambda.min
print(best_lambda)
# We then predict for the test set and calculate the MSE
predlasso1 <- predict(lasso.cv, newx = lasso_x[-training_set,], s = lasso.cv$lambda.min)
predMSElasso <- mean((lasso_y[-training_set] - predlasso1)^2)
print(predMSElasso)
rss <- sum((predlasso1 - lasso_y[-training_set])^2) #residual sum of squares
tss <- sum((lasso_y[-training_set] - mean(lasso_y[-training_set])) ^ 2)
rsq <- 1 - rss/tss
print(rsq)
lasso2.cv <- cv.glmnet(
lasso_x,
lasso_y,
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso2 <- coef(lasso2.cv, s = "lambda.min") # save for later comparison
print(coef_lasso2) #From a visual inspection, this does not seem to add any additional explanatory power
corrplot(cor(mydata), method = "color")
pairs(mydata, upper.panel = NULL, pch = 20, cex = 1.25) # assumption: linear relationship between Education and Examination/Examination and Agriculture
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Agriculture, y = Fertility)) +
geom_smooth(mapping = aes(x = Agriculture, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Examination, y = Fertility)) +
geom_smooth(mapping = aes(x = Examination, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
xlab("Examination") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Examination,
y = Fertility,
color = Examination,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Examination,
y = Fertility),
method = "lm")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
xlab("Education") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Education,
y = Fertility,
color = Education,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Education,
y = Fertility),
method = "lm") +
ylim(0, 100)
ggplot(mydata, aes(x = Education, y = Fertility)) +
geom_bin2d() +
theme_bw()
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Catholic, y = Fertility)) +
geom_smooth(mapping = aes(x = Catholic, y = Fertility),
method = "lm") # Two regions as either high in catholic or low
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Infant.Mortality, y = Fertility)) +
geom_smooth(mapping = aes(x = Infant.Mortality, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm")
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 50)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 40)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Education") +
geom_point(mapping = aes(
x = Agriculture,
y = Education,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Education),
method = "lm")
mydata %>%
ggplot() +
xlab("Examination") +
ylab("Education") +
geom_point(mapping = aes(
x = Examination,
y = Education,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Examination,
y = Education),
method = "lm")
library(datasets.load)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lattice)
library(plotly)
library(knitr)
library(MASS)
library(faraway)
library(moments)
library(stargazer)
library(leaps)
library(glmnet)
library(np)
library(lmtest)
library(mctest)
library(faraway)
library(lattice)
library(olsrr)
library(corrplot)
library(GGally)
library(ggcorrplot)
library(hrbrthemes)
library(viridis)
help(swiss)
View(swiss)
mydata = swiss[, 1:6]
View(mydata)
#### Re-naming the relevant variables for easier access ####
attach(mydata)
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
step1 <- lm(Fertility ~ 1, data = mydata) # using only the intercept
step2 <- lm(Fertility ~ ., data = mydata) # running the full regression
# Next, we run the forward and backward regression #
step(step1, direction = "forward", scope = list(lower = step1, upper = step2))
step(step2, direction = "backward") # Both methods yield the same result: the best model does not include the variable "Examination"
# We then plot the results for better visual inspection #
model_eva <- leaps::regsubsets(Fertility ~ ., nbest = 2, data = mydata) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which) # it seems to become evident that including both variables Education and Examination decreases the goodness of the model
# We can further visualize the BIC factor and AdjR2
par(mfrow = c(1, 2))
plot(model_eva, main ="Comparison of goodness") # The lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
ols_plot_diagnostics(reg_interact) # Residuals seem normally distributed and no signs of heteroskedasticity
ols_plot_diagnostics(reg_interact) # Residuals seem normally distributed and no signs of heteroskedasticity
resi_interact_transformed = reg_interact_transformed$residuals
shapiro.test(resi_interact_transformed) # The p-value of 0.5451 supports normal distribution of residuals
resi_interact = reg_interact$residuals
shapiro.test(resi_interact) # Large p-value of 0.3102 supports normal distribution of residuals
bptest(reg_interact) # No signs of heteroskedasticity
resettest(reg_interact, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
resettest(reg_interact, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
library(datasets.load)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lattice)
library(plotly)
library(knitr)
library(MASS)
library(faraway)
library(moments)
library(stargazer)
library(leaps)
library(glmnet)
library(np)
library(lmtest)
library(mctest)
library(faraway)
library(lattice)
library(olsrr)
library(corrplot)
library(GGally)
library(ggcorrplot)
library(hrbrthemes)
library(viridis)
mydata = swiss[, 1:6]
attach(mydata)
stargazer(mydata,
type = "html",
nobs = FALSE,
style = "aer",
iqr = FALSE ,
title = "Table 1 - Swiss Fertility Summary Statistics",
digits = 2,
out = "Summary Statistics")
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
Education2 = Education^2
Education3 = Education^3
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
reg_woEx = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
reg_interact = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact)
reg_interact_transformed = lm(log(Fertility) ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact_transformed)
swiss$CatholicDummy = ifelse(swiss$Catholic >= 50, 1, 0)
View(swiss)
# Then, we can run the respective simple and multivariate regression models again, including the Catholic Dummy Variable #
reg_simple_Dummy = lm(Fertility ~ Education + CatholicDummy, data = swiss)
summary(reg_simple_Dummy)
reg_woEx_Dummy = lm(Fertility ~ Agriculture + Education + Infant.Mortality + CatholicDummy, data = swiss)
summary(reg_woEx_Dummy)
stargazer(reg_simple, reg_simple_transformed, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_interact, reg_interact_transformed, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTable.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(2, 6, 2),
add.lines = list(c("Model", "Linear", "Log Y", "Quadratic", "Cubic", "Full", "w/o Exam.", "Interact", "Int. / Log Y", "Dummy", "Dummy Full"),
c("AIC", round(AIC(reg_simple), 1), "", round(AIC(reg_simple2), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), "", round(AIC(reg_simple_Dummy), 1), round(AIC(reg_woEx_Dummy), 1)),
c("BIC", round(BIC(reg_simple), 1), "", round(BIC(reg_simple2), 1), round(BIC(reg_simple3), 1), round(BIC(reg_full), 1), round(BIC(reg_woEx), 1), round(BIC(reg_interact), 1), "", round(BIC(reg_simple_Dummy), 1), round(BIC(reg_woEx_Dummy), 1))),
notes = "SE provided in parentheses",
results = "asis")
stargazer(reg_simple, reg_simple3, reg_full, reg_woEx, reg_interact, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(1, 4, 1),
add.lines = list(c("AIC", round(AIC(reg_simple), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), round(AIC(reg_woEx_Dummy), 1))),
omit.stat = c("rsq", "n", "ser"),
df = F,
notes = "Based on 47 observations. SE provided in parentheses",
results = "asis")
stargazer(mydata,
type = "html",
out = "SummaryStatistics.html",
nobs = FALSE,
style = "aer",
iqr = FALSE,
title = "Table 1 - Swiss Fertility Summary Statistics",
digits = 2,
median = TRUE,
results = "asis")
