library(glmnet)
library(np)
library(lmtest)
library(mctest)
library(faraway)
library(lattice)
library(olsrr)
library(corrplot)
library(GGally)
library(ggcorrplot)
library(hrbrthemes)
library(viridis)
help(swiss)
View(swiss)
mydata = swiss[, 1:6]
View(mydata)
attach(mydata)
summary(mydata) # Comment: Catholic: Median and Mean are completely different, also high sd (41)
stargazer(mydata,
type = "html",
nobs = FALSE,
style = "aer",
iqr = FALSE ,
title = "Table 1 - Swiss Fertility Summary Statistics",
digits = 2,
out = "Summary Statistics")
boxplot(mydata, ylab = "Frequency", main = "Boxplot of the Swiss Fertility data set")
ggplot_data_frame = data.frame(
name = c(rep("Fertility", 47), rep("Agriculture", 47), rep("Examination", 47), rep("Education", 47), rep("Catholic", 47), rep("Infant.Mortality", 47)),
value = c(as.vector(mydata$Fertility), as.vector(mydata$Agriculture), as.vector(mydata$Examination), as.vector(mydata$Education), as.vector(mydata$Catholic), as.vector(mydata$Infant.Mortality))
)
ggplot_data_frame$name = factor(ggplot_data_frame$name, levels = c("Fertility", "Agriculture", "Examination", "Education", "Catholic", "Infant.Mortality"))
ggplot_data_frame%>%
ggplot(aes(x = name, y = value, fill = name)) +
geom_boxplot() +
scale_fill_viridis(discrete = TRUE, alpha = 0.6, option = "A") +
geom_jitter(color = "black", size = 0.7, alpha = 0.9) +
theme_ipsum() +
theme(
legend.position = "none",
plot.title = element_text(size = 11)
) +
ggtitle("Jitter boxplot of Swiss dataset") +
xlab("")
ggplot_data_frame%>%
ggplot(aes(x = name, y = value, fill = name)) +
geom_violin() +
scale_fill_viridis(discrete = TRUE, alpha = 0.6, option = "A") +
theme_ipsum() +
theme(
legend.position = "none",
plot.title = element_text(size = 11)
) +
ggtitle("Violin chart of Swiss dataset") +
xlab("")
plot.ecdf(Fertility, xlab = "Fertility", main = "Empirical Distribution Function Fertility")
plot.ecdf(Agriculture, xlab = "Agriculture", main ="Empirical Distribution Function Agriculture")
plot.ecdf(Examination, xlab = "Examination", main = "Empirical Distribution Function Examination")
plot.ecdf(Catholic, xlab = "Catholic", main = "Empirical Distribution Function Catholic") # Comment: looks almost binary
plot.ecdf(Infant.Mortality, xlab = "Infant.Mortality", main = "Empirical Distribution Function Infant Mortality")
hist(Education, main = "Education", xlab = "Education") # Mostly lower levels of education in the dataset
hist(Fertility, main = "Fertility", xlab = "Fertility") # Fertility rates are mostly between 60 and 90%
plot(density(Fertility), main = "Fertility")
abline(v=c(mean(Fertility), median(Fertility)), col="gray94") # Median and Mean close, almost normally distributed with some tail
ggplot(mydata, aes(x = Fertility)) +
geom_histogram(aes(y = ..density..), colour = "black", fill = "grey", binwidth = 5) +
geom_density(alpha = .2, fill = "lightgrey") +
geom_vline(aes(xintercept = mean(Fertility)),
color = "red", linetype = "dashed", size = 1) +
labs(title = "Fertility histogram plot", xlab = "Fertility", ylab = "Density")
ggplot(mydata, aes(x = Education)) +
geom_histogram(aes(y = ..density..), colour = "black", fill = "lightblue", binwidth = 5) +
geom_density(alpha = .2, fill = "blue") +
geom_vline(aes(xintercept = mean(Education)),
color = "red", linetype = "dashed", size = 1) +
labs(title = "Education histogram plot", xlab = "Education", ylab = "Density")
cov(mydata)
cor_matrix = as.matrix(cor(mydata)) # correlations with response variable lower than .8, therefore no signs of strong multicollinearity
cor_matrix[upper.tri(cor_matrix)] <- NA
print(cor_matrix, na.print = "")
ggcorrplot(cor(mydata), hc.order = TRUE, # another way of visualizing the correlation matrix
type = "lower",
digits = 3,
lab = TRUE,
lab_size = 4,
method = "square",
colors = c("tomato2", "white", "springgreen3"),
title = "Correlogram Swiss Data Set",
ggtheme = theme_bw)
ggpairs(mydata, upper = list(continuos = wrap("cor", size = 12)))
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
head(10)
mydata %>%
dplyr::select(Fertility) %>%
arrange(desc(Fertility)) %>%
tail(10) # Cities: Geneve, Lausanne, Nyone
mydata %>%
dplyr::select(Education) %>%
arrange(desc(Education)) %>%
head(10) # Geneve 53 (outlier)
mydata %>%
dplyr::select(Education) %>%
arrange(desc(Education)) %>%
tail(10)
reg_simple = lm(Fertility ~ Education, data = mydata)
summary(reg_simple)
fit = fitted(reg_simple)
plot(Education, Fertility, main = "Fertility and Education Regression", xlab = "Education", ylab = "Fertility")
lines(Education, fit, col = 2)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
ggtitle("Fertility and Education Regression") +
xlab("Education") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Education,
y = Fertility,
color = Education,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Education,
y = Fertility),
method = "lm") +
ylim(0, 100)
ols_plot_obs_fit(reg_simple) # The fit does not seem completely off, but could be further improved
par(mfrow = c(2,2))
plot(reg_simple)
ols_plot_diagnostics(reg_simple)
#
resi_simple = reg_simple$residuals
plot(mydata$Education, resi_simple, main = "Residuals from the linear regression (Fertility ~ Education)", xlab = "Education", ylab = "Residuals")
lines(mydata$Education, rep(0, times = length(mydata$Education)), col = 2) # no clear pattern can be observed in the residuals (can only be said for lower levels of Education given amount of datapoints)
par(mfrow = c(1,1))
resi_simple = reg_simple$residuals
plot(mydata$Education, resi_simple, main = "Residuals from the linear regression (Fertility ~ Education)", xlab = "Education", ylab = "Residuals")
lines(mydata$Education, rep(0, times = length(mydata$Education)), col = 2) # no clear pattern can be observed in the residuals (can only be said for lower levels of Education given amount of datapoints)
plot(density(resi_simple), main = "Residuals for the Simple Linear Regression Model")
shapiro.test(resi_simple) # p-value of 0.0592 so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_simple) #p-value of 0.5252
boxcox(reg_simple)
reg_simple_transformed = lm(log(Fertility) ~ Education, data = mydata)
summary(reg_simple_transformed)
ols_plot_diagnostics(reg_simple_transformed) # The residuals still seem to follow a normal distribution and there are no signs of heteroskedasticity
resi_simple_transformed = reg_simple_transformed$residuals
shapiro.test(resi_simple_transformed) # we do not reject the 0 hypo of normal distribution in the errors
bptest(reg_simple_transformed) # given the large p-value of 0.5559, there seems to be no heteroskedasticity
reg_simple_transformed_2 = lm(Fertility ~ log(Education), data = mydata)
summary(reg_simple_transformed_2) # no improvement, therefore no log transformation of independent variable needed
ZV = lm(log(Fertility) ~ log(Education), data = mydata)
summary(ZV)
Education2 = Education^2
Education3 = Education^3
cor(Education, Education2) # highly correlated 0.9361279
cor(Education, Education3) # rel. highly correlated 0.8389176
cor(Education2, Education3) # highly correlated 0.9734444
reg_simple2 <- lm(Fertility ~ Education + Education2)
reg_simple3 <- lm(Fertility ~ Education + Education2 + Education3)
summary(reg_simple2) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
summary(reg_simple3) # the very large p-values do not indicate a statistically significant impact of Education on Fertility for any power
fit_matrix <- matrix(NA, nrow = 3, ncol = 2)
colnames(fit_matrix) <- c("R2", "AdjR2")
rownames(fit_matrix) <- c("Linear", "Quadratic", "Cubic")
fit_matrix[1,1] <- summary(reg_simple)$r.squared
fit_matrix[1,2] <- summary(reg_simple)$adj.r.squared
fit_matrix[2,1] <- summary(reg_simple2)$r.squared
fit_matrix[2,2] <- summary(reg_simple2)$adj.r.squared
fit_matrix[3,1] <- summary(reg_simple3)$r.squared
fit_matrix[3,2] <- summary(reg_simple3)$adj.r.squared
fit_matrix # we can see that the linear model offers the highest Adj. R squared
plot(Education, reg_simple3$fitted.values, col = "blue", ylab = "Fitted Values", main = "Comparison of Value Fit for Polynomial Regression")
points(Education, reg_simple2$fitted.values, col = "black")
lines(Education, reg_simple$fitted.values, col = "red") # the small deviations of fit indicates that the linear model is sufficient to use
bandwidth <- npregbw(Fertility ~ Education)
summary(bandwidth) # the optimal bandwidth is 6.76351
npreg1 <- npreg(bws = bandwidth)
plot(bandwidth, plot.errors.method = "bootstrap", main = "Non-parametric regression")
lines(Education, reg_simple$fitted.values, col = "red")
k <- 9
sigma_simple <- vector(length = k)
sigma_simple2 <- vector(length = k)
sigma_simple3 <- vector(length = k)
for(i in 1:k){
ind <- rep(0, times = 47)
a <- (i - 1) * 5 + 1
b <- i * 5
ind[a : b] = 1
Test.F = swiss$Fertility[ind == 1]
Test.E = swiss$Education[ind == 1]
Train.F = swiss$Fertility[ind == 0]
Train.E = swiss$Education[ind == 0]
# Defining the test sets
Train.E2 = Train.E^2
Train.E3 = Train.E^3
#Fitting the training data
reg_linear <- lm(Train.F ~ Train.E)
reg_quadratic <- lm(Train.F ~ Train.E + Train.E2)
reg_cubic <- lm(Train.F ~ Train.E + Train.E2 + Train.E3)
#Extract the fitted values
coef.1 <- reg_linear$coef
coef.2 <- reg_quadratic$coef
coef.3 <- reg_cubic$coef
#Calculate SSR on dataset
sigma_simple[i] <- sum((Test.F - coef.1[1] - coef.1[2] * Test.E)^2)
sigma_simple2[i] <- sum((Test.F - coef.2[1] - coef.2[2] * Test.E - coef.2[3] * (Test.E^2))^2)
sigma_simple3[i] <- sum((Test.F - coef.3[1] - coef.3[2] * Test.E - coef.3[3] * (Test.E^2) - coef.3[4] * (Test.E^3))^2)
}
# We then calculate the average SSR for all three models #
avg_sigma_simple <- mean(sigma_simple)
avg_sigma_simple2 <- mean(sigma_simple2)
avg_sigma_simple3 <- mean(sigma_simple3)
avg_sigma_simple
avg_sigma_simple2
avg_sigma_simple3
# Finally, we create  a matrix to store the values #
cross_validation_matrix <- matrix(NA, nrow = 3, ncol = 1)
colnames(cross_validation_matrix) <- c("Average SSR")
rownames(cross_validation_matrix) <- c("Linear", "Quadratic", "Cubic")
cross_validation_matrix[1] <- avg_sigma_simple
cross_validation_matrix[2] <- avg_sigma_simple2
cross_validation_matrix[3] <- avg_sigma_simple3
cross_validation_matrix
resettest(reg_simple, power = 2:3, type = "regressor") # p-value of 61.2% suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
reg_full = lm(Fertility ~ Education + Agriculture + Examination + Catholic + Infant.Mortality, data = mydata)
summary(reg_full) # Education is still highly significant from a statistical standpoint, however Examination does not seem to have a significant influence on Fertility
ols_plot_obs_fit(reg_full) # The model seems to fit pretty well based on this visual inspection
ols_plot_diagnostics(reg_full) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_full) > 0.1,] # Porrentruy, Sierre, Neuchatel, Rive Droite and Rive Gauche are influential points according to the Cook's Distance parameter
resi_full = reg_full$residuals
shapiro.test(resi_full) # p-value of 0.9318, so we cannot reject the 0 hypothesis of normal distribution
bptest(reg_full) #p-value of 0.321 lends support for homoskedasticity in the model
ols_vif_tol(reg_full) # Based on the given values which are all below threshold of 4 / 5 and especially 10, there does not seem to be a problem with multicollinearity in the model
resettest(reg_full, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
step1 <- lm(Fertility ~ 1, data = mydata) # using only the intercept
step2 <- lm(Fertility ~ ., data = mydata) # running the full regression
step(step1, direction = "forward", scope = list(lower = step1, upper = step2))
step(step2, direction = "backward") # Both methods yield the same result: the best model does not include the variable "Examination"
model_eva <- leaps::regsubsets(Fertility ~ ., nbest = 2, data = mydata) # number of best models per number of included variables
print(summary(model_eva))
print(summary(model_eva)$which) # it seems to become evident that including both variables Education and Examination decreases the goodness of the model
par(mfrow = c(1, 2))
plot(model_eva, main ="Comparison of goodness") # The lower the BIC, the better the model. The  best model includes Agriculture, Education, Catholic and Infant.Mortality
plot(model_eva, scale = "adjr2") # The highest and second highest adj. R2 are achieved by the full model and by excluding Examination
ols_plot_added_variable(reg_full) # The variable Examination does not seem to have a large contribution to the model
reg_woEx = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality, data = mydata)
summary(reg_woEx)
ols_plot_obs_fit(reg_woEx) # again, the fit seems to be pretty well
ols_plot_diagnostics(reg_woEx) # The residuals seem to be normally distributed and no heteroskedasticity present
mydata[cooks.distance(reg_woEx) > 0.1,] # Only Porrentruy, Sierre and Rive Gauche are now influential points according to the Cook's Distance parameter
resi_woEx = reg_woEx$residuals
shapiro.test(resi_woEx) # large p value, therefore normal distribution of residuals
bptest(reg_woEx) # high p value, therefore no heteroskedasticity
ols_vif_tol(reg_woEx) # Unsurprisingly, there is no multicollinearity here either
resettest(reg_woEx, power = 2:3, type = "regressor") # p-value of 0.6311 suggests that adding second and third order of the regressor makes no statistically significant contribution to the model
boxcox(reg_woEx) # The model seems to be accurately approximated, no need for transformation of the dependent variable visible
reg_interact = lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact)
ols_plot_diagnostics(reg_interact) # Residuals seem normally distributed and no signs of heteroskedasticity
resi_interact = reg_interact$residuals
shapiro.test(resi_interact) # Large p-value of 0.3102 supports normal distribution of residuals
bptest(reg_interact) # No signs of heteroskedasticity
ols_vif_tol(reg_interact) # Only interaction term that shows some weak signs of multicollinearity, therefore no cause for concern
resettest(reg_interact, power = 2:3, type = "regressor") # Large p-value does not indicate a need for polynomial transformation of any regressors
boxcox(reg_interact) # Based on visual inspection (low Lambda value), we decide to transform the dependent variable using the log
reg_interact_transformed = lm(log(Fertility) ~ Education + Agriculture + Catholic + Infant.Mortality + Education:Catholic, data = mydata)
summary(reg_interact_transformed)
ols_plot_obs_fit(reg_interact_transformed) # Fit seems to be rather good for this model
ols_plot_diagnostics(reg_interact_transformed) # Residuals seem normally distributed and no signs of heteroskedasticity
shapiro.test(resi_interact_transformed) # The p-value of 0.5451 supports normal distribution of residuals
bptest(reg_interact_transformed) # No signs of heteroskedasticity
resettest(reg_interact_transformed, power = 2:3, type = "regressor") # Large p-value does also not indicate a need for polynomial transformation of any regressors
resi_interact_transformed = reg_interact_transformed$residuals
shapiro.test(resi_interact_transformed) # The p-value of 0.5451 supports normal distribution of residuals
bptest(reg_interact_transformed) # No signs of heteroskedasticity
resettest(reg_interact_transformed, power = 2:3, type = "regressor") # Large p-value does also not indicate a need for polynomial transformation of any regressors
swiss$CatholicDummy = ifelse(swiss$Catholic >= 50, 1, 0)
View(swiss)
reg_simple_Dummy = lm(Fertility ~ Education + CatholicDummy, data = swiss)
summary(reg_simple_Dummy)
reg_woEx_Dummy = lm(Fertility ~ Agriculture + Education + Infant.Mortality + CatholicDummy, data = swiss)
summary(reg_woEx_Dummy)
stargazer(reg_simple, reg_simple_transformed, reg_simple2, reg_simple3, reg_full, reg_woEx, reg_interact, reg_interact_transformed, reg_simple_Dummy, reg_woEx_Dummy,
type = "html",
out = "RegressionTable.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(2, 6, 2),
add.lines = list(c("Model", "Linear", "Log Y", "Quadratic", "Cubic", "Full", "w/o Exam.", "Interact", "Int. / Log Y", "Dummy", "Dummy Full"),
c("AIC", round(AIC(reg_simple), 1), "", round(AIC(reg_simple2), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), "", round(AIC(reg_simple_Dummy), 1), round(AIC(reg_woEx_Dummy), 1)),
c("BIC", round(BIC(reg_simple), 1), "", round(BIC(reg_simple2), 1), round(BIC(reg_simple3), 1), round(BIC(reg_full), 1), round(BIC(reg_woEx), 1), round(BIC(reg_interact), 1), "", round(BIC(reg_simple_Dummy), 1), round(BIC(reg_woEx_Dummy), 1))),
notes = "SE provided in parentheses",
results = "asis")
stargazer(reg_simple, reg_simple3, reg_full, reg_woEx, reg_interact, reg_woEx_Dummy,
type = "html",
out = "RegressionTableShort.html",
digits = 3,
header = FALSE,
align = TRUE,
no.space = TRUE,
title = "Regression Analysis",
intercept.bottom = FALSE,
dep.var.caption = "Impact on Fertility",
dep.var.labels.include = FALSE,
covariate.labels = c("Intercept", "Education", "Education<sup>2</sup>", "Education<sup>3</sup>", "Agriculture", "Examination", "Catholic", "Infant.Mortality", "Education x Catholic", "CatholicDummy"),
column.labels = c("Simple", "Multivariate", "Dummy"),
column.separate = c(1, 4, 1),
add.lines = list(c("AIC", round(AIC(reg_simple), 1), round(AIC(reg_simple3), 1), round(AIC(reg_full), 1), round(AIC(reg_woEx), 1), round(AIC(reg_interact), 1), round(AIC(reg_woEx_Dummy), 1))),
omit.stat = c("rsq", "n", "ser"),
df = F,
notes = "Based on 47 observations. SE provided in parentheses",
results = "asis")
set.seed (20210508)
lasso_x <- as.matrix(mydata[ ,2:6])
lasso_y <- swiss$Fertility
size <- floor(0.75 * nrow(mydata)) # Generate variable with the rows in training data
training_set <- sample(seq_len(nrow(mydata)), size = size)
lasso.cv <- cv.glmnet(
lasso_x[training_set,],
lasso_y[training_set],
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso <- coef(lasso.cv, s = "lambda.min") # save for later comparison
print(coef_lasso) # It seems like none have to be dropped to be compared to the OLS
plot(lasso.cv) # Grey bars are standard deviations along the Lambda sequence
best_lambda <- lasso.cv$lambda.min
print(best_lambda)
# We then predict for the test set and calculate the MSE
predlasso1 <- predict(lasso.cv, newx = lasso_x[-training_set,], s = lasso.cv$lambda.min)
predMSElasso <- mean((lasso_y[-training_set] - predlasso1)^2)
print(predMSElasso)
rss <- sum((predlasso1 - lasso_y[-training_set])^2) #residual sum of squares
tss <- sum((lasso_y[-training_set] - mean(lasso_y[-training_set])) ^ 2)
rsq <- 1 - rss/tss
print(rsq)
lasso2.cv <- cv.glmnet(
lasso_x,
lasso_y,
type.measure = "mse",
family = "gaussian", # non binary
nfolds = 10, # number of folds
alpha = 1 # alpha = 1 means ridge penalty =0 and only lasso penalty remains (inbetween is a mix)
)
coef_lasso2 <- coef(lasso2.cv, s = "lambda.min") # save for later comparison
print(coef_lasso2) #From a visual inspection, this does not seem to add any additional explanatory power
corrplot(cor(mydata), method = "color")
pairs(mydata, upper.panel = NULL, pch = 20, cex = 1.25) # assumption: linear relationship between Education and Examination/Examination and Agriculture
levelplot(cor(mydata), xlab = "", ylab = "") # visible correlation between Fertility and Education & Examination. Also, highly negative correlation between Education and Agriculture
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Agriculture, y = Fertility)) +
geom_smooth(mapping = aes(x = Agriculture, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Examination, y = Fertility)) +
geom_smooth(mapping = aes(x = Examination, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
xlab("Examination") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Examination,
y = Fertility,
color = Examination,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Examination,
y = Fertility),
method = "lm")
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Education, y = Fertility)) +
geom_smooth(mapping = aes(x = Education, y = Fertility),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
xlab("Education") +
ylab("Fertility") +
geom_point(mapping = aes(
x = Education,
y = Fertility,
color = Education,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Education,
y = Fertility),
method = "lm") +
ylim(0, 100)
ggplot(mydata, aes(x = Education, y = Fertility)) +
geom_bin2d() +
theme_bw()
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Catholic, y = Fertility)) +
geom_smooth(mapping = aes(x = Catholic, y = Fertility),
method = "lm") # Two regions as either high in catholic or low
mydata %>%
ggplot() +
geom_point(mapping = aes(x = Infant.Mortality, y = Fertility)) +
geom_smooth(mapping = aes(x = Infant.Mortality, y = Fertility),
method = "lm")
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm")
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 100)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 50)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Examination") +
geom_point(mapping = aes(
x = Agriculture,
y = Examination,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Examination),
method = "lm") +
ylim(0, 40)
mydata %>%
ggplot() +
xlab("Agriculture") +
ylab("Education") +
geom_point(mapping = aes(
x = Agriculture,
y = Education,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Agriculture,
y = Education),
method = "lm")
mydata %>%
ggplot() +
xlab("Examination") +
ylab("Education") +
geom_point(mapping = aes(
x = Examination,
y = Education,
color = Fertility,
size = Fertility,
alpha = 0.5)) +
geom_smooth(mapping = aes(
x = Examination,
y = Education),
method = "lm")
